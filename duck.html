<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Head and Gaze Tracking with 3D Duck</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/facemesh"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
</head>
<body>
    <h1>Head and Gaze Tracking with 3D Duck</h1>
    <video id="video" width="640" height="480" autoplay></video>
    <canvas id="output" width="640" height="480"></canvas>
    <div id="3d-duck" style="width: 640px; height: 480px;"></div>

    <script>
        async function setupCamera() {
            const video = document.getElementById('video');
            const stream = await navigator.mediaDevices.getUserMedia({ video: true });
            video.srcObject = stream;
            return new Promise(resolve => {
                video.onloadedmetadata = () => {
                    resolve(video);
                };
            });
        }

        async function loadModels() {
            const faceModel = await blazeface.load();
            const meshModel = await facemesh.load();
            return { faceModel, meshModel };
        }

        async function detect(video, faceModel, meshModel, duck) {
            const canvas = document.getElementById('output');
            const context = canvas.getContext('2d');
            //context.clearRect(0, 0, canvas.width, canvas.height);
            const predictions = await faceModel.estimateFaces(video, false);
            const meshPredictions = await meshModel.estimateFaces(video, false);

            if (predictions.length > 0) {
                context.drawImage(video, 0, 0, canvas.width, canvas.height);
                predictions.forEach(prediction => {
                    const keypoints = prediction.landmarks;
                    context.fillStyle = 'red';
                    keypoints.forEach(([x, y]) => {
                        context.fillRect(x, y, 5, 5);
                    });
                });

                if (meshPredictions.length > 0) {
                    const headRotation = meshPredictions[0].faceInViewConfidence; // Simplified example
                    duck.rotation.x = headRotation * Math.PI / 180;
                    duck.rotation.y = headRotation * Math.PI / 180;
                }
            }

            requestAnimationFrame(() => detect(video, faceModel, meshModel, duck));
        }

        function init3DDuck() {
            const scene = new THREE.Scene();
            const camera = new THREE.PerspectiveCamera(75, 640 / 480, 0.1, 1000);
            const renderer = new THREE.WebGLRenderer();
            renderer.setSize(640, 480);
            document.getElementById('3d-duck').appendChild(renderer.domElement);

            const geometry = new THREE.BoxGeometry();
            const material = new THREE.MeshBasicMaterial({ color: 0x00ff00 });
            const duck = new THREE.Mesh(geometry, material);
            scene.add(duck);

            camera.position.z = 5;

            const animate = function () {
                requestAnimationFrame(animate);
                renderer.render(scene, camera);
            };
            animate();

            return duck;
        }

        async function main() {
            const video = await setupCamera();
            const { faceModel, meshModel } = await loadModels();
            const duck = init3DDuck();
            detect(video, faceModel, meshModel, duck);
        }

        main();
    </script>
</body>
</html>
